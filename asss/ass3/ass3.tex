
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{url}
\usepackage{tikz}
\usepackage[nomarkers,notablist,nofiglist]{endfloat}
\usepackage{multirow, array}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Monday, July 12, 2021 22:56:44}
%TCIDATA{LastRevised=Sunday, July 18, 2021 21:46:01}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}

\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}


\begin{document}

\title{ \textsc{COMM8102 Econometric Analysis} \\
\textsc{Assignment 3}}
\author{\textsc{Yuhao Liu} \\
%EndAName
{\small z5097536}}
\date{{\small \today}}
\maketitle

\section*{Exercise 7.1}

Let $\hat{\beta}_{1}$ be the estimator of $\beta _{1}$ by only regressing $Y$
on $X_{1}$.

\begin{eqnarray*}
\hat{\beta}_{1} &=&\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{1i}^{\prime
}\right) ^{-1}\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}Y_{i}\right)  \\
&=&\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{1i}^{\prime }\right) ^{-1}\left( 
\frac{1}{n}\sum_{i=1}^{n}X_{1i}\left( X_{1i}^{\prime }\beta
_{1}+X_{2i}^{\prime }\beta _{2}+e_{i}\right) \right)  \\
&=&\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{1i}^{\prime }\right) ^{-1}\left( 
\frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{1i}^{\prime }\beta _{1}+\frac{1}{n}%
\sum_{i=1}^{n}X_{1i}X_{2i}^{\prime }\beta _{2}+\frac{1}{n}%
\sum_{i=1}^{n}X_{1i}e_{i}\right)  \\
&=&\beta _{1}+\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{1i}^{\prime }\right)
^{-1}\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{2i}^{\prime }\beta _{2}\right)
+\left( \frac{1}{n}\sum_{i=1}^{n}X_{1i}X_{1i}^{\prime }\right) ^{-1}\left( 
\frac{1}{n}\sum_{i=1}^{n}X_{1i}e_{i}\right)  \\
&&\overset{p}{\rightarrow }\beta _{1}+E\left[ X_{1}X_{1}^{\prime }\right]
^{-1}E\left[ X_{1}X_{2}^{\prime }\right] \beta _{2}+E\left[
X_{1}X_{1}^{\prime }\right] ^{-1}E\left[ X_{1}e\right]  \\
&=&\beta _{1}+E\left[ X_{1}X_{1}^{\prime }\right] ^{-1}E\left[
X_{1}X_{2}^{\prime }\right] \beta _{2}+0 \\
&=&\beta _{1}+E\left[ X_{1}X_{1}^{\prime }\right] ^{-1}E\left[
X_{1}X_{2}^{\prime }\right] \beta _{2}
\end{eqnarray*}

by WLLN and CMT. $E\left[ X_{1}e\right] =0,$ since $E\left[ Xe\right] =0$.
Because $\hat{\beta}_{1}\overset{p}{\rightarrow }\beta _{1}+E\left[
X_{1}X_{1}^{\prime }\right] ^{-1}E\left[ X_{1}X_{2}^{\prime }\right] \beta
_{2}$, $\hat{\beta}_{1}$ is not consistent for $\beta _{1}.$ When $E\left[
X_{1}X_{2}^{\prime }\right] =\mathbf{0}$ or $\beta _{2}=\mathbf{0}$, $\hat{%
\beta}_{1}\overset{p}{\rightarrow }\beta _{1}$.

\section*{Exercise 7.15}

\begin{eqnarray*}
\sqrt{n}\left( \hat{\beta}-\beta \right)  &=&\sqrt{n}\left( \frac{%
\sum_{i=1}^{n}X_{i}^{3}Y_{i}}{\sum_{i=1}^{n}X_{i}^{4}}-\beta \right)  \\
&=&\sqrt{n}\left( \frac{\sum_{i=1}^{n}X_{i}^{3}\left( X_{i}\beta
+e_{i}\right) }{\sum_{i=1}^{n}X_{i}^{4}}-\beta \right)  \\
&=&\sqrt{n}\left( \beta +\frac{\sum_{i=1}^{n}X_{i}^{3}e_{i}}{%
\sum_{i=1}^{n}X_{i}^{4}}-\beta \right)  \\
&=&\frac{\sqrt{n}\left( \sum_{i=1}^{n}X_{i}^{3}e_{i}-\underset{0}{%
\underbrace{E\left[ X_{i}^{3}e\right] }}\right) }{\sum_{i=1}^{n}X_{i}^{4}} \\
&&\overset{d}{\rightarrow }\frac{\mathcal{N}\left( 0,E\left[
X_{i}^{6}e_{i}^{2}\right] \right) }{E\left[ X_{i}^{4}\right] } \\
&=&\mathcal{N}\left( 0,\frac{E\left[ X_{i}^{6}e_{i}^{2}\right] }{\left( E%
\left[ X_{i}^{4}\right] \right) ^{2}}\right) 
\end{eqnarray*}

by CLT and CMT. 

\begin{eqnarray*}
E\left[ X_{i}^{3}e\right]  &=&E\left[ E\left[ \left. X_{i}^{3}e\right\vert
X_{i}\right] \right]  \\
&=&E\left[ X_{i}^{3}E\left[ \left. e\right\vert X_{i}\right] \right]  \\
&=&0
\end{eqnarray*}

\section*{Exercise 9.4}

\subsection*{(a)}

The size of a test is the probability of a false rejection of the (true)
null hypothesis:

\begin{eqnarray*}
P\left( \left. \text{Reject }H_{0}\right\vert H_{0}\text{ is true}\right)
&=&P\left( \left. \text{Reject }H_{0}\right\vert H_{0}\right) \\
&=&P\left( \left. W<c_{1} \text{ or } W>c_{2}\right\vert H_{0}\right)
\end{eqnarray*}

Under $H_{0},$ $W\overset{d}{\rightarrow }\chi _{q}^{2},$ $P\left( \left.
W<c_{1}\text{ or }W>c_{2}\right\vert H_{0}\right) =P\left( \left.
W<c_{1}\right\vert H_{0}\right) +P\left( \left. W>c_{2}\right\vert
H_{0}\right) =\frac{\alpha }{2}+1-(1-\frac{\alpha }{2})=\alpha .$

\subsection*{(b)}

This is not a good test of $H_{0}~vs~H_{1}.$Wald test is the extension of
t-test to multivariate case. Intuitively, Wald test statistic is the sqaured
version of t statistic in the scalar case. Only the right tail captures the
extreme deviation from $0$ (both positive and negative). This can also be
extended to the multivarite case. We do not need to look at the left tail of
the $\chi _{q}^{2}$ distribution. A one-side test is enough. 

\section*{Exercise 9.12}

This interpretation is not correct.

The power of a hypothesis test is the probability that reject $H_{0}$ when
the $H_{1}$ is true. $p$ value is a quantity depending on the value of test
statistic and the asymptotic null distribution $G.$ \ The power depends on
the critical value and the distribution of test statistic under the
alternative hypothesis.

Although a larger sample will make the test have more power, it does not
necessarily reduce the $p$ value. $p$ value depends on both of the test
statistic and the asymptotic null distribution $G.$ The extra samples may
make the value of test statistic increase, but it may also make the value of
test statistic decrease. Therefore, it cannot be guranteed that a larger
sample will make the null rejected.

\section*{Exercise 9.16}

\subsection*{(a)}

\begin{equation*}
\hat{\theta}=\frac{1}{n}\sum_{i=1}^{n}e_{1i}^{2}-\frac{1}{n}%
\sum_{i=1}^{n}e_{2i}^{2}
\end{equation*}

\subsection*{(b)}

By central limit theorem,

\begin{equation*}
\left( 
\begin{array}{c}
\sqrt{n}\left( \frac{1}{n}\sum_{i=1}^{n}e_{1i}^{2}-\sigma _{1}^{2}\right) \\ 
\sqrt{n}\left( \frac{1}{n}\sum_{i=1}^{n}e_{2i}^{2}-\sigma _{2}^{2}\right)%
\end{array}%
\right) \overset{d}{\longrightarrow }\mathcal{N}\left( \left( 
\begin{array}{c}
0 \\ 
0%
\end{array}%
\right) ,V\right) ,
\end{equation*}

where

\begin{equation*}
V=\left( 
\begin{array}{cc}
E\left[ \left( e_{1}^{2}-\sigma _{1}^{2}\right) ^{2}\right] & E\left[ \left(
e_{1}^{2}-\sigma _{1}^{2}\right) \left( e_{2}^{2}-\sigma _{2}^{2}\right) %
\right] \\ 
E\left[ \left( e_{1}^{2}-\sigma _{1}^{2}\right) \left( e_{2}^{2}-\sigma
_{2}^{2}\right) \right] & E\left[ \left( e_{2}^{2}-\sigma _{2}^{2}\right)
^{2}\right]%
\end{array}%
\right) .
\end{equation*}

Define a function $r:\mathbb{R}^{2}\rightarrow \mathbb{R},$ such that $%
r\left( \sigma _{1}^{2},\sigma _{2}^{2}\right) =$ $\sigma _{1}^{2}-\sigma
_{2}^{2}.$ Then we have $\theta =r\left( \sigma _{1}^{2},\sigma
_{2}^{2}\right) ,\hat{\theta}=r\left( \frac{1}{n}\sum_{i=1}^{n}e_{1i}^{2},%
\frac{1}{n}\sum_{i=1}^{n}e_{2i}^{2}\right) .$ By delta method,

\begin{eqnarray*}
\sqrt{n}\left( \hat{\theta}-\theta \right)  &=&\sqrt{n}\left( r\left( \frac{1%
}{n}\sum_{i=1}^{n}e_{1i}^{2},\frac{1}{n}\sum_{i=1}^{n}e_{2i}^{2}\right)
-r\left( \sigma _{1}^{2},\sigma _{2}^{2}\right) \right) , \\
&&\overset{d}{\longrightarrow }\mathcal{N}\left( 0,R^{\prime }VR\right) ,
\end{eqnarray*}

where

\begin{eqnarray*}
R &=&\left( 
\begin{array}{c}
\frac{\partial }{\partial \sigma _{1}^{2}}r\left( \sigma _{1}^{2},\sigma
_{2}^{2}\right)  \\ 
\frac{\partial }{\partial \sigma _{2}^{2}}r\left( \sigma _{1}^{2},\sigma
_{2}^{2}\right) 
\end{array}%
\right) , \\
&=&\left( 
\begin{array}{c}
1 \\ 
-1%
\end{array}%
\right) .
\end{eqnarray*}

Then we have 

\begin{eqnarray*}
R^{\prime }VR &=&E\left[ \left( e_{1}^{2}-\sigma _{1}^{2}\right) ^{2}\right]
-2\cdot E\left[ \left( e_{1}^{2}-\sigma _{1}^{2}\right) \left(
e_{2}^{2}-\sigma _{2}^{2}\right) \right] +E\left[ \left( e_{2}^{2}-\sigma
_{2}^{2}\right) ^{2}\right] , \\
&=&E\left[ \left( \left( e_{1}^{2}-\sigma _{1}^{2}\right) -\left(
e_{2}^{2}-\sigma _{2}^{2}\right) \right) ^{2}\right] , \\
&=&E\left[ \left( e_{1}^{2}-e_{2}^{2}-\left( \sigma _{1}^{2}-\sigma
_{2}^{2}\right) \right) ^{2}\right] .
\end{eqnarray*}

Finally, 

\begin{equation*}
\sqrt{n}\left( \hat{\theta}-\theta \right) \overset{d}{\longrightarrow }%
\mathcal{N}\left( 0,E\left[ \left( e_{1}^{2}-e_{2}^{2}-\left( \sigma
_{1}^{2}-\sigma _{2}^{2}\right) \right) ^{2}\right] \right) .
\end{equation*}

\subsection*{(c)}

Since $\hat{\theta}=\theta +\frac{\sqrt{n}\left( \hat{\theta}-\theta \right) 
}{\sqrt{n}},$ and $\sqrt{n}\left( \hat{\theta}-\theta \right) \overset{d}{%
\longrightarrow }\mathcal{N}\left( 0,E\left[ \left(
e_{1}^{2}-e_{2}^{2}-\left( \sigma _{1}^{2}-\sigma _{2}^{2}\right) \right)
^{2}\right] \right) $,

\begin{equation*}
\hat{\theta}\overset{d}{\rightarrow }\mathcal{N}\left( \theta ,\frac{E\left[
\left( e_{1}^{2}-e_{2}^{2}-\left( \sigma _{1}^{2}-\sigma _{2}^{2}\right)
\right) ^{2}\right] }{n}\right) 
\end{equation*}

An estimator of the asymptotic variance of $\hat{\theta}$ is

\begin{equation*}
\hat{V}_{\hat{\theta}}=\frac{1}{n}\left( \frac{1}{n}\sum_{i=1}^{n}\left(
e_{1i}^{2}-e_{2i}^{2}-\left( \sigma _{1}^{2}-\sigma _{2}^{2}\right) \right)
^{2}\right) .
\end{equation*}

\subsection*{(d)}

$H_{0}$ is equivalent to $\theta =0$, and $H_{1}$ is equivalent to $\theta
\neq 0.$ Under $H_{0}$ the test statistic is

\begin{eqnarray*}
T_{n} &=&\frac{\hat{\theta}-0}{\sqrt{\hat{V}_{\hat{\theta}}}} \\
&\sim &\mathcal{N}\left( 0,1\right) .
\end{eqnarray*}

A test of asymptotic size $\alpha $ can be that we reject the null
hypothesis if $T_{n}<a_{1}$ or $T_{n}>a_{2}$, where $a_{1}$ is the $\frac{%
\alpha }{2}$ quantile of $\mathcal{N}\left( 0,1\right) $ and $a_{2}$ is the $%
1-\frac{\alpha }{2}$\ of $\mathcal{N}\left( 0,1\right) .$

\subsection*{(e)}

Acceptance of the null hypothesis means these two models have same variance
for the residuals. therefore they fit the data equally well.

\section*{Exercise 9.26}

\subsection*{(a)}

\begin{center}
\begin{tabular}{ccc}
\hline\hline
& Estimations & Standard errors $HC1$ \\ \cline{2-3}
$\hat{\beta}_{1}$ & $-3.527$ & $1.7186$ \\ 
$\hat{\beta}_{2}$ & $0.720$ & $0.0326$ \\ 
$\hat{\beta}_{3}$ & $0.436$ & $0.2456$ \\ 
$\hat{\beta}_{4}$ & $-0.220$ & $0.3238$ \\ 
$\hat{\beta}_{5}$ & $0.427$ & $0.0755$ \\ \hline\hline
\end{tabular}
\end{center}

\subsection*{(b)}

\begin{eqnarray*}
\log C &=&\beta _{1}+\beta _{2}\log Q+\beta _{3}\log PL+\beta _{4}\log
PK+\beta _{5}\log PF+e \\
&=&\beta _{1}+\log Q^{\beta _{2}}+\log PL^{\beta _{3}}+\log PK^{\beta
_{4}}+\log PF^{\beta _{5}}+e
\end{eqnarray*}

Then we have

\begin{equation*}
C=\exp \left( \beta _{1}\right) \cdot \exp \left( e\right) \cdot Q^{\beta
_{2}}\cdot PL^{\beta _{3}}\cdot PK^{\beta _{4}}\cdot PF^{\beta _{5}}
\end{equation*}

For $\beta _{3}+\beta _{4}+\beta _{5}=1$, this means the cost function has a
constant return to scale for labor, capital and fuel.

\subsection*{(e)}

Define a function $r:\mathbb{R}^{5}\rightarrow \mathbb{R}$, such that $%
r\left( \beta _{1},\beta _{2},\beta _{3},\beta _{4},\beta _{5}\right) =\beta
_{3}+\beta _{4}+\beta _{5}$. The we have $\nabla r=\left[ 0,0,1,1,1\right]
^{\prime }$. By Delta method and affine transform of normal distribution, we
establish

\begin{equation*}
\sqrt{n}\left( \hat{\beta}_{3}+\hat{\beta}_{4}+\hat{\beta}_{5}-\left( \beta
_{3}+\beta _{4}+\beta _{5}\right) \right) \overset{d}{\longrightarrow }%
\mathcal{N}\left( 0,V_{r}\right) 
\end{equation*}

where $V_{r}=\nabla r^{\prime }\mathbf{V}_{\beta }\nabla r$. $\mathbf{V}%
_{\beta }$ is the asymptotic covariance matrix of $\sqrt{n}\left( \mathbf{%
\hat{\beta}-\beta }\right) .$

Then the Wald statistic is

\begin{eqnarray*}
\mathcal{W}_{n} &=&\frac{\left( \hat{\beta}_{3}+\hat{\beta}_{4}+\hat{\beta}%
_{5}-\left( \beta _{3}+\beta _{4}+\beta _{5}\right) \right) ^{2}}{\hat{V}_{%
\hat{r}}^{HC1}} \\
&=&0.645.
\end{eqnarray*}

Since $\mathcal{W}_{n}\overset{d}{\longrightarrow }\chi _{1}^{2}$, the
critical value $c=3.84$ for $\alpha =0.05.$ We do not reject the null
hypothesis at $5\%$ significance level.

\end{document}
